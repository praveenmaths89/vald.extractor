}
print("✅ --- TRIAL data pull complete! ---")
# --- 5. Combine and Save ---
print("Combining all trial chunks...")
if (length(all_trial_chunks) > 0) {
# Combine all the small trial chunks into one big dataframe
all_trials_data <- dplyr::bind_rows(all_trial_chunks)
print(paste("Total trial rows fetched:", nrow(all_trials_data)))
# --- SAVE THE DATA ---
rdata_file_path <- "vald_database.Rdata"
# We save the 'all_tests_data' (from Step 3) and the new 'all_trials_data'
save(all_tests_data, all_trials_data, file = rdata_file_path)
print("-------------------------------------------------")
print(paste("✅ SUCCESS: Your data has been saved to:", rdata_file_path))
print("You can now use the 'Weekly Refresh' script for future updates.")
print("-------------------------------------------------")
} else {
print("Error: No trial data was successfully fetched. File not saved.")
}
# --- 6. Clean Up ---
valdr::set_start_date(NULL)
# ============================================
# 10. PREPARE METADATA (Handle List-Columns)
# ============================================
cat("Preparing metadata for joining...\n")
# ===================================================================
# 4. DATA PROCESSING: JOIN AND CLEAN
# ===================================================================
cat("Joining tests and trials data...\n")
# Combine the two data frames into one master data frame
# This joins by "testId" and "athleteId"
all_data <- left_join(all_trials_data, all_tests_data)
# ===================================================================
cat("Transforming data to wide format...\n")
colnames(all_data)
test_data_agg <- all_data %>%
mutate(
value  = as.numeric(value),
weight = as.numeric(weight)
) %>%
# --- IMPROVED FILTER ---
filter(
is.finite(value),
value > 0.01,           # Removes near-zero 'ghost' triggers
!is.na(definition_name) # Ensures the metric is actually named
) %>%
# -----------------------
group_by(
athleteId, testId, testType, recordedUTC,
recordedDateOffset, trialLimb, definition_name
) %>%
summarise(
mean_result = median(value, na.rm = TRUE),
mean_weight = median(weight, na.rm = TRUE),
rep_count   = n(),
.groups = "drop"
)# --- Step 3b: Create the "Testdate" column ---
# 'mean_weight' is now passed through this step
test_data_with_date <- test_data_agg %>%
mutate(
TestTimestampUTC = ymd_hms(recordedUTC),
TestTimestampLocal = TestTimestampUTC + minutes(recordedDateOffset),
Testdate = as.Date(TestTimestampLocal)
)
library(tidyr)
library(dplyr)
# --- Step 3c: Pivot to the Wide Format ---
structured_test_data <- test_data_with_date %>%
# *** 3. ADDED 'mean_weight' to the select ***
select(athleteId, Testdate, testId, testType, trialLimb, definition_name, mean_result, mean_weight) %>%
pivot_wider(
# *** 4. ADDED 'mean_weight' to id_cols (so it's kept as a column) ***
id_cols = c(athleteId, Testdate, testId, mean_weight),
names_from = c(definition_name, trialLimb, testType), # testType at the end
values_from = mean_result,
names_glue = "{definition_name}_{trialLimb}_{testType}" # e.g., "PEAK_FORCE_Both_CMJ"
) %>%
# Rename the aggregated weight column to be clear
rename(Weight_on_Test_Day = mean_weight)
cat("Wide data successfully created.\n")
str(structured_test_data)
# ===================================================================
# 4. FILTER FOR CMJ VARIABLES (***UPDATED***)
# ===================================================================
cat("Filtering wide data for CMJ variables only...\n")
# First, find a key metric to filter by.
# We'll find the first column name that ends with "_CMJ"
# (e.g., "PEAK_FORCE_Both_CMJ")
key_cmj_metric <- names(structured_test_data)[grepl("_CMJ$", names(structured_test_data))][1]
if (is.na(key_cmj_metric)) {
stop("Error: No CMJ columns found in the data.")
}
cat("Using key metric '", key_cmj_metric, "' to filter rows.\n", sep="")
# We select the ID columns and any column that ends with "_CMJ"
cmj_data_wide <- structured_test_data %>%
# *** 1. ADDED FILTER STEP ***
# Keep only rows where our key CMJ metric is NOT empty (NA)
# This removes all other test types (like Drop Jump, etc.)
filter(!is.na(!!sym(key_cmj_metric))) %>%
# *** 2. SELECT STEP (same as before) ***
select(
athleteId,
Testdate,
testId,
Weight_on_Test_Day,
ends_with("_CMJ") # This selects all your CMJ columns
)
cat("Filtered CMJ data frame created. Rows:", nrow(cmj_data_wide), "\n")
#########################################################################################
# Required libraries
library(httr)
library(dplyr)
library(jsonlite)   # for robust JSON parsing
library(data.table) # optional, if you want rbindlist later
print("Script starting...")
# ---------- CONFIG / CREDENTIALS ----------
MY_CLIENT_ID     <- "uLL8avmYG1XpnoEQ=="
MY_CLIENT_SECRET <- "hoVxRTWgC670xQvRFTfxvDHjNElxUYxXXg="   # rotate this if it's real
MY_TENANT_ID     <- "5c46eae1-72b5-11ea-8817-431cb039e378"
# Use the security host you intended (your script used security.valdperformance.com)
token_url       <- "https://security.valdperformance.com/connect/token"
profile_base    <- "https://prd-aue-api-externalprofile.valdperformance.com"
tenants_base    <- "https://prd-aue-api-externaltenants.valdperformance.com"
# ---------- 1. Get access token (safe parsing & error checks) ----------
message("Getting access token...")
token_response <- POST(
url = token_url,
body = list(
grant_type    = "client_credentials",
client_id     = MY_CLIENT_ID,
client_secret = MY_CLIENT_SECRET
),
encode = "form",
accept_json()
)
if (status_code(token_response) != 200) {
stop(sprintf("Error: Could not get access token. Status: %s\nResponse: %s",
status_code(token_response),
content(token_response, as = "text", encoding = "UTF-8")))
}
# parse token robustly
token_txt <- content(token_response, as = "text", encoding = "UTF-8")
token_json <- tryCatch(fromJSON(token_txt, simplifyVector = FALSE), error = function(e) NULL)
access_token <- if (!is.null(token_json$access_token)) token_json$access_token else NULL
if (is.null(access_token) || access_token == "") {
stop("Error: access_token not found in token response. Response body:\n", token_txt)
}
message("✅ Access token obtained.")
auth_header <- add_headers(Authorization = paste("Bearer", access_token), Accept = "application/json")
# ---------- 2. Get master list of profiles ----------
message("--- PROFILES: Fetching basic profile list... ---")
list_api_url <- paste0(profile_base, "/profiles")
get_response <- GET(
url = list_api_url,
query = list(TenantId = MY_TENANT_ID), # note capital T to match Swagger
auth_header
)
# Required libraries
library(httr)
library(dplyr)
library(jsonlite)   # for robust JSON parsing
library(data.table) # optional, if you want rbindlist later
print("Script starting...")
# ---------- CONFIG / CREDENTIALS ----------
MY_CLIENT_ID     <- "uLL8avmYG1XpnoEQ=="
MY_CLIENT_SECRET <- "hoVxRTWgC670xQvRFTfxvDHjNElxUYxXXg="   # rotate this if it's real
MY_TENANT_ID     <- "5c46eae1-72b5-11ea-8817-431cb039e378"
# Use the security host you intended (your script used security.valdperformance.com)
token_url       <- "https://security.valdperformance.com/connect/token"
profile_base    <- "https://prd-aue-api-externalprofile.valdperformance.com"
tenants_base    <- "https://prd-aue-api-externaltenants.valdperformance.com"
# ---------- 1. Get access token (safe parsing & error checks) ----------
message("Getting access token...")
token_response <- POST(
url = token_url,
body = list(
grant_type    = "client_credentials",
client_id     = MY_CLIENT_ID,
client_secret = MY_CLIENT_SECRET
),
encode = "form",
accept_json()
)
if (status_code(token_response) != 200) {
stop(sprintf("Error: Could not get access token. Status: %s\nResponse: %s",
status_code(token_response),
content(token_response, as = "text", encoding = "UTF-8")))
}
# parse token robustly
token_txt <- content(token_response, as = "text", encoding = "UTF-8")
token_json <- tryCatch(fromJSON(token_txt, simplifyVector = FALSE), error = function(e) NULL)
access_token <- if (!is.null(token_json$access_token)) token_json$access_token else NULL
if (is.null(access_token) || access_token == "") {
stop("Error: access_token not found in token response. Response body:\n", token_txt)
}
message("✅ Access token obtained.")
auth_header <- add_headers(Authorization = paste("Bearer", access_token), Accept = "application/json")
# ---------- 2. Get master list of profiles ----------
message("--- PROFILES: Fetching basic profile list... ---")
list_api_url <- paste0(profile_base, "/profiles")
get_response <- GET(
url = list_api_url,
query = list(TenantId = MY_TENANT_ID), # note capital T to match Swagger
auth_header
)
if (status_code(get_response) != 200) {
stop(sprintf("Error: Could not get profile list. Status: %s\n%s",
status_code(get_response),
content(get_response, as = "text", encoding = "UTF-8")))
}
# robust parsing of possibly wrapped responses
profiles_txt <- content(get_response, as = "text", encoding = "UTF-8")
profiles_json <- tryCatch(fromJSON(profiles_txt, simplifyVector = FALSE), error = function(e) NULL)
# handle either { "profiles": [...] } or [...] style responses
if (!is.null(profiles_json$profiles)) {
all_profiles_list <- profiles_json$profiles
} else if (is.list(profiles_json) && length(profiles_json) > 0) {
# sometimes API returns a top-level object with "profiles" or directly an array; handle both
all_profiles_list <- profiles_json
} else {
all_profiles_list <- list()
}
message(sprintf("✅ Found %d profiles in master list.", length(all_profiles_list)))
# ---------- 3. Loop to fetch full details for each profile ----------
full_profile_details <- list()
profile_api_url <- paste0(profile_base, "/profiles/")
message(sprintf("Starting to fetch full details for %d profiles...", length(all_profiles_list)))
if (length(all_profiles_list) > 0) {
for (i in seq_along(all_profiles_list)) {
profile_item <- all_profiles_list[[i]]
# the summary may contain profileId or id; handle both:
profile_id <- profile_item$profileId %||% profile_item$id %||% NULL
if (is.null(profile_id)) {
warning("Skipping item with missing profile id at index ", i)
next
}
current_url <- paste0(profile_api_url, profile_id)
response <- GET(url = current_url,
query = list(tenantId = MY_TENANT_ID),
auth_header)
if (status_code(response) == 200) {
# parse JSON
resp_txt <- content(response, as = "text", encoding = "UTF-8")
parsed <- tryCatch(fromJSON(resp_txt, simplifyVector = FALSE), error = function(e) {
warning("Failed to parse profile JSON for ", profile_id, ": ", e$message)
NULL
})
full_profile_details[[profile_id]] <- parsed
} else {
warning("Failed fetching profile ", profile_id, " status: ", status_code(response))
}
Sys.sleep(0.25) # respect rate limit
}
}
message("✅ Profile looping complete!")
message("Total profiles successfully fetched: ", length(full_profile_details))
# ---------- 4. Convert profile list to dataframe ----------
message("Converting profile list to dataframe...")
if (length(full_profile_details) > 0) {
Profile_meta <- data.table::rbindlist(full_profile_details, use.names = TRUE, fill = TRUE, idcol = "profileId_from_list")
Profile_meta <- as.data.frame(Profile_meta)
message("✅ 'Profile_meta' Dataframe Created! Rows: ", nrow(Profile_meta))
} else {
message("No profile details fetched; 'Profile_meta' not created.")
}
# ---------- 5. Groups: fetch master list ----------
message("--- GROUPS: Fetching basic group list... ---")
groups_list_api_url <- paste0(tenants_base, "/groups")
get_groups_response <- GET(
url = groups_list_api_url,
query = list(TenantId = MY_TENANT_ID),
auth_header
)
if (status_code(get_groups_response) != 200) {
stop(sprintf("Error: Could not get group list. Status: %s\n%s",
status_code(get_groups_response),
content(get_groups_response, as = "text", encoding = "UTF-8")))
}
groups_txt <- content(get_groups_response, as = "text", encoding = "UTF-8")
groups_json <- tryCatch(fromJSON(groups_txt, simplifyVector = FALSE), error = function(e) NULL)
if (!is.null(groups_json$groups)) {
all_groups_list <- groups_json$groups
} else if (is.list(groups_json) && length(groups_json) > 0) {
all_groups_list <- groups_json
} else {
all_groups_list <- list()
}
message(sprintf("✅ Found %d groups in master list.", length(all_groups_list)))
# ---------- 6. Loop to fetch full details for groups ----------
full_group_details <- list()
groups_api_url <- paste0(tenants_base, "/groups/")
message(sprintf("Starting to fetch full details for %d groups...", length(all_groups_list)))
if (length(all_groups_list) > 0) {
for (i in seq_along(all_groups_list)) {
grp_item <- all_groups_list[[i]]
grp_id <- grp_item$id %||% grp_item$groupId %||% NULL
if (is.null(grp_id)) {
warning("Skipping group with missing id at index ", i)
next
}
current_url <- paste0(groups_api_url, grp_id)
response <- GET(url = current_url, query = list(tenantId = MY_TENANT_ID), auth_header)
if (status_code(response) == 200) {
resp_txt <- content(response, as = "text", encoding = "UTF-8")
parsed <- tryCatch(fromJSON(resp_txt, simplifyVector = FALSE), error = function(e) {
warning("Failed to parse group JSON for ", grp_id, ": ", e$message)
NULL
})
full_group_details[[grp_id]] <- parsed
} else {
warning("Failed fetching group ", grp_id, " status: ", status_code(response))
}
Sys.sleep(0.25)
}
}
message("✅ Group looping complete!")
message("Total groups successfully fetched: ", length(full_group_details))
# ---------- 7. Convert group list to dataframe ----------
message("Converting group list to dataframe...")
if (length(full_group_details) > 0) {
Groups_meta <- data.table::rbindlist(full_group_details, use.names = TRUE, fill = TRUE, idcol = "groupId_from_list")
Groups_meta <- as.data.frame(Groups_meta)
message("✅ 'Groups_meta' Dataframe Created! Rows: ", nrow(Groups_meta))
} else {
message("No group details fetched; 'Groups_meta' not created.")
}
message("--- SCRIPT FINISHED ---")
###############################################################################################
library(dplyr)
library(tidyr)
library(data.table)
# Safety: ensure Profile_meta, Groups_meta, structured_test_data exist
if (!exists("Profile_meta")) stop("Profile_meta not found.")
if (!exists("Groups_meta")) stop("Groups_meta not found.")
if (!exists("structured_test_data")) message("Warning: structured_test_data not found yet. Final join will error unless created.")
# --- Normalize Groups_meta: ensure groupId and name exist ---
if ("id" %in% names(Groups_meta) && !("groupId" %in% names(Groups_meta))) {
Groups_meta <- Groups_meta %>% rename(groupId = id)
}
if (!("groupId" %in% names(Groups_meta))) stop("Groups_meta must contain 'groupId' or 'id'. Found: ", paste(names(Groups_meta), collapse = ", "))
if (!("name" %in% names(Groups_meta))) Groups_meta$name <- NA_character_
# --- Determine which column is the profile id in Profile_meta ---
possible_profile_id_cols <- c("profileId", "id", "profileId_from_list", "ProfileId")
profile_id_col <- intersect(possible_profile_id_cols, names(Profile_meta))[1]
if (is.null(profile_id_col)) {
stop("No profile id column found in Profile_meta. Expected one of: ", paste(possible_profile_id_cols, collapse = ", "))
}
message("Using profile id column: ", profile_id_col)
# --- 1. Expand profiles so each row has a single groupId (if any) ---
# Ensure groupIds column exists; if missing, create empty list-column
if (!("groupIds" %in% names(Profile_meta))) {
Profile_meta$groupIds <- vector("list", nrow(Profile_meta))
}
# Unnest groupIds safely (handle single values or lists)
profiles_long_df <- Profile_meta %>%
mutate(
# create a character-based profileId column from detected id column
profileId = as.character(.data[[profile_id_col]]),
# ensure groupIds is a list-column (unnest_longer needs list or atomic vector)
groupIds = lapply(groupIds, function(x) if (is.null(x)) NA_character_ else x)
) %>%
tidyr::unnest_longer(groupIds, keep_empty = TRUE) %>%
mutate(groupIds = as.character(groupIds))
# --- 2. Prepare groups df for join ---
groups_df <- Groups_meta %>%
mutate(groupId = as.character(groupId)) %>%
select(groupId, name, everything())
# --- 3. Join profiles -> groups using matching id columns ---
merged_df <- profiles_long_df %>%
left_join(groups_df, by = c("groupIds" = "groupId"))
# Save merged for inspection
write.csv(merged_df, file = "~/Downloads/merged_df.csv", row.names = FALSE)
message("Saved: ~/Downloads/merged_df.csv")
# --- 4. Create unique_athletes_df: one row per athlete with all groups concatenated ---
# Define identity columns safely: pick columns that exist, else create NA
identity_cols_wanted <- c("profileId", "givenName", "familyName", "dateOfBirth", "sex", "email", "weightInKg", "heightInCm")
identity_cols <- intersect(identity_cols_wanted, names(merged_df))
# add missing identity columns as NA so group_by won't error
missing_identity <- setdiff(identity_cols_wanted, identity_cols)
if (length(missing_identity) > 0) {
for (col in missing_identity) merged_df[[col]] <- NA
identity_cols <- identity_cols_wanted
}
unique_athletes_df <- merged_df %>%
group_by(across(all_of(identity_cols))) %>%
summarise(
all_group_names = {
v <- unique(na.omit(name))
if (length(v) == 0) NA_character_ else paste(v, collapse = ", ")
},
all_group_ids = {
v <- unique(na.omit(groupIds))
if (length(v) == 0) NA_character_ else paste(v, collapse = ", ")
},
.groups = "drop"
)
write.csv(unique_athletes_df, file = "~/Downloads/unique_athletes_df.csv", row.names = FALSE)
message("Saved: ~/Downloads/unique_athletes_df.csv")
# --- 5. Join athlete metadata to test data ---
if (exists("structured_test_data")) {
# detect athlete id column in structured_test_data
possible_athlete_id_cols <- c("profileId", "athleteId", "AthleteId")
athlete_id_col <- intersect(possible_athlete_id_cols, names(structured_test_data))[1]
if (is.null(athlete_id_col)) stop("structured_test_data must contain 'profileId' or 'athleteId' column.")
# create consistent profileId column for join
structured_test_data <- structured_test_data %>%
mutate(profileId = as.character(.data[[athlete_id_col]]))
final_analysis_data <- structured_test_data %>%
left_join(unique_athletes_df %>% mutate(profileId = as.character(profileId)), by = "profileId")
write.csv(final_analysis_data, file = "~/Downloads/final_analysis_data.csv", row.names = FALSE)
message("Saved: ~/Downloads/final_analysis_data.csv")
} else {
message("structured_test_data not found; skipped final join.")
}
message("Pipeline complete.")
###########################################################################################################
table(final_analysis_data$all_group_names)
###########################################################################################################
table(final_analysis_data$sex)
library(readxl)
library(dplyr)
# Read sheet 2 from your Excel file in Downloads
gender_fix <- read_excel("Gender_Missing.xlsx", sheet = 1)
getwd()
# --- 0. Load Required Libraries ---
options(timeout = 18000) # 30 minute timeout
library(valdr)
library(dplyr)
library(readr)
library(lubridate)
# --- 1. Your Credentials ---
client_id     <- "uLL8avmYG1XpnoEQ=="
client_secret <- "hoVxRTWgC670xQvRFTfxvDHjNElxUYxXXg="
tenant_id     <- "5c46eae1-72b5-11ea-8817-431cb039e378"
region        <- "aue"
# Set VALD credentials
valdr::set_credentials(
client_id     = client_id,
client_secret = client_secret,
tenant_id     = tenant_id,
region        = region
)
print("✅ Credentials set.")
# --- 2. Set the Start Date ---
start_date_str <- "2020-01-01T00:00:00Z"
valdr::set_start_date(start_date_str)
print(paste("Data pull start date set to:", start_date_str))
# --- 3. Data Pull (Step 1: Get ALL Tests) ---
print("--- Starting data pull for TESTS... ---")
print("This may take several minutes...")
tryCatch({
all_tests_data <- valdr::get_forcedecks_tests_only()
print(paste("✅ --- TEST data pull complete! ---"))
print(paste("Total test rows fetched:", nrow(all_tests_data)))
}, error = function(e) {
print("!!! TEST DATA PULL FAILED !!!")
print(e$message)
stop("Halting script. Could not retrieve test data.")
})
# --- 4. Data Pull (Step 2: Get TRIALS in Chunks) ---
print("--- Starting data pull for TRIALS (in chunks)... ---")
# --- Chunking Setup ---
chunk_size <- 100 # We will pull trials for 100 tests at a time
total_tests <- nrow(all_tests_data)
# Create a sequence of chunk start rows: 1, 101, 201, 301, etc.
chunk_starts <- seq(1, total_tests, by = chunk_size)
# Create an empty list to store all the small trial dataframes
all_trial_chunks <- list()
print(paste("Splitting", total_tests, "tests into", length(chunk_starts), "chunks of", chunk_size))
# --- Loop Through Each Chunk ---
for (i in 1:length(chunk_starts)) {
start_row <- chunk_starts[i]
end_row   <- min(start_row + chunk_size - 1, total_tests) # Make sure we don't go past the end
print(paste("... Fetching trials for test rows", start_row, "to", end_row))
tryCatch({
# 4a. Get the small chunk of tests
current_test_chunk <- all_tests_data[start_row:end_row, ]
# 4b. Pull the trials for ONLY that small chunk
current_trial_chunk <- valdr::get_forcedecks_trials_only(current_test_chunk)
# 4c. Add the results to our list
if (nrow(current_trial_chunk) > 0) {
all_trial_chunks[[i]] <- current_trial_chunk
}
}, error = function(e) {
print(paste("!!! ERROR on chunk", i, "(rows", start_row, "-", end_row, ") !!!"))
print(e$message)
# We continue to the next chunk instead of stopping
})
}
print("✅ --- TRIAL data pull complete! ---")
# --- 5. Combine and Save ---
print("Combining all trial chunks...")
if (length(all_trial_chunks) > 0) {
# Combine all the small trial chunks into one big dataframe
all_trials_data <- dplyr::bind_rows(all_trial_chunks)
print(paste("Total trial rows fetched:", nrow(all_trials_data)))
# --- SAVE THE DATA ---
rdata_file_path <- "vald_database.Rdata"
# We save the 'all_tests_data' (from Step 3) and the new 'all_trials_data'
save(all_tests_data, all_trials_data, file = rdata_file_path)
print("-------------------------------------------------")
print(paste("✅ SUCCESS: Your data has been saved to:", rdata_file_path))
print("You can now use the 'Weekly Refresh' script for future updates.")
print("-------------------------------------------------")
} else {
print("Error: No trial data was successfully fetched. File not saved.")
}
# --- 6. Clean Up ---
valdr::set_start_date(NULL)
setwd("~/Downloads/project 23")
# 1. Restart R (close and reopen RStudio)
q()
